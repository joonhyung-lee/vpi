<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="`VPI` project page">
  <meta property="og:title"
    content="Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation" />
  <meta property="og:description" content="details about the paper `VPI`" />
  <meta property="og:url" content="https://joonhyung-lee.github.io/vpi/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="assets/images/fig-overview.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="assets/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    .image-row {
      display: flex; /* Use flexbox to create a row layout */
      flex-wrap: wrap; /* Allow items to wrap to the next line if needed */
      justify-content: space-between; /* Space items evenly along the row */
    }
    .image {
      margin-bottom: 20px; /* Add some space between images */
      flex-basis: calc(25% - 20px); /* Set the initial size of each image to occupy 25% of the container minus the margin */
      max-width: calc(25% - 20px); /* Limit the maximum width of each image */
      flex-grow: 1; /* Allow images to grow to fill the available space */
      flex-shrink: 1; /* Allow images to shrink if necessary */
    }
    figure {
      text-align: center; /* Center the contents of the figure */
    }
  </style>

  <title>Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation
  </title>
  <link rel="icon" type="image/x-icon" href="assets/images/rilab.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="assets/css/bulma.min.css">
  <link rel="stylesheet" href="assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="assets/css/bulma-slider.min.css">
  <link rel="stylesheet" href="assets/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="assets/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="assets/js/fontawesome.all.min.js"></script>
  <script src="assets/js/bulma-carousel.min.js"></script>
  <script src="assets/js/bulma-slider.min.js"></script>
  <script src="assets/js/index.js"></script>

  <!-- Include MathJax Library -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  <!-- model viewer -->
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>

  <!-- MathJax Configuration -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['$$', '$$']],
        processEscapes: true,
      },
      "HTML-CSS": { availableFonts: ["TeX"] },
      showMathMenu: false,
    });
  </script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Visual Preference Inference: An Image Sequence-Based Preference
              Reasoning in Tabletop Object Manipulation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://joonhyung-lee.github.io/" target="_blank">Joonhyung Lee</a><sup>1</sup>,
                <span class="author-block">
                  <a href="https://park-sangbeom.github.io/" target="_blank">Sangbeom Park</a><sup>1</sup>,
                  <span class="author-block">Yongin Kwon</a><sup>2</sup>,
            </div>
            <div class="is-size-5 publication-authors">

              <span class="author-block">
                Jemin Lee</a><sup>2</sup>,
                <span class="author-block">
                  Minwook Ahn</a><sup>3</sup>,
                  <span class="author-block">
                    <a href="https://sites.google.com/view/sungjoon-choi/home?authuser=0" target="_blank">Sungjoon
                      Choi</a><sup>1</sup>
                  </span>
            </div>

            <div class="is-size-5 publication-authors" style="text-align: center;">
              <span class="author-block">Korea University<sup>1</sup><br></span>,
              <span class="author-block">ETRI<sup>2</sup><br></span>,
              <span class="author-block">Neubla<sup>3</sup><br></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2403.11513" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="assets/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/joonhyung-lee/vpi" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

                <!-- Video Link -->
                <span class="link-block">
                  <a href="https://github.com/joonhyung-lee/vpi" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <hr>

  <!-- Teaser video-->
  <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="assets/videos/SPOTS_final_s4.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle" style="text-align: left;">
        <span style="font-weight: bold; font-size: larger">SPOTS</span>
        is an approach to a semi-autonomous teleoperation framework that focuses on verifying placement positions with 1) a <span style="font-weight: bold;">Stability Verification</span> (i.e., physics-based simulation) step and 
        2) <span style="font-weight: bold;">Receptacle Reasoning</span> (i.e., common knowledge) step by utilizing LLMs that understand scene contexts and reason about the corresponding task without learning.
      </h2>
    </div>
  </div>
</section> -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              In robotic object manipulation, human preferences can often be influenced by the visual attributes of
              objects, such as color and shape.
              These properties play a crucial role in operating a robot to interact with objects and align with human
              intention.
              In this paper, we focus on the problem of inferring underlying human preferences from a sequence of raw
              visual observations in tabletop manipulation environments with a variety of object types, named Visual
              Preference Inference (VPI).
              To facilitate visual reasoning in the context of manipulation, we introduce the Chain-of-Visual-Residuals
              (CoVR) method. CoVR employs a prompting mechanism that describes the difference between the consecutive
              images (i.e., visual residuals) and incorporates such texts with a sequence of images to infer the user's
              preference.
              This approach significantly enhances the ability to understand and adapt to dynamic changes in its visual
              environment during manipulation tasks. Furthermore, we incorporate such texts along with a sequence of
              images to infer the user's preferences.
              Our method outperforms baseline methods in terms of extracting human preferences from visual sequences in
              both simulation and real-world environments.
              <!-- Code, data, and details are available at: <a href="https://joonhyung-lee.github.io/vpi/" target="_blank">https://joonhyung-lee.github.io/vpi/</a> -->
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Gradio -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container" style="max-width: 90%; margin: auto;"> <!-- Adjust the width here -->
        <h2 class="title is-3">Running Example</h2>
        <div id="framework" style="text-align: center;">
          <div class="content has-text-justified">
            <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.19.2/gradio.js"></script>
            <gradio-app src="https://joonh-robotics-vpi.hf.space" style="width: 100%;"></gradio-app>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End of Gradio -->
  <!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Running Example</h2>
    <div class="content has-text-justified">
      <script 
        type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.19.2/gradio.js">
      </script>
      <gradio-app
        src="https://joonh-robotics-vpi.hf.space">
      </gradio-app>  
    </div>
  </div>
</section> -->


  <!-- Framework Overview -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Framework</h2>
        <div id="framework" style="text-align: center;">
          <a href="#framework">
            <img src="assets/images/fig-overview.png" alt="MY ALT TEXT" width="100%" />
          </a>
          <h2 class="content has-text-justified" style="text-align: left;">
            <span style="font-weight: bold;">Overview of Chain-of-Visual-Residuals:</span> (a) We introduce a Visual
            Preference Inference (VPI) task, which extracts users' preferences solely from visual representations in
            tabletop manipulation environments.
            Our approach, CoVR prompting, involves generating (b) visual reasoning descriptions of consecutive images
            and (c) chaining these descriptions for interpreting human preferences from the scene sequences.
          </h2>
        </div>
      </div>
    </div>
  </section>
  <!-- End of Framework Overview -->

  <!-- Module Explanations -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">\( \textbf{VPI} \)</h2>
        <div id="framework" style="text-align: center;">
          <img src="assets/images/fig-vpi-task.png" alt="MY ALT TEXT" width="40%" />
          <h2 class="content has-text-justified" style="text-align: left;">
            <span style="font-weight: bold;">Visual Preference Inference (VPI) Tasks:</span>
            We define a task of VPI as reasoning user preference based on an image sequence.
            Specifically, the task involves a robot that moves objects to target locations, following user instructions
            via mouse clicks which provide which object to move and where to place it.
            To infer the user's preferences, we extract visual residuals from each image in the sequence and link them
            together to enhance reasoning capability.
          </h2>
          <br>
        </div>

        <div id="module" class="columns">
          <!-- First Column -->
          <div class="column is-half">
            <h2 class="title is-3">Visual Reasoning Descriptor</h2>
            <div class="item item-video1">
              <figure class="video-container">
                <img src="assets/images/fig-vrd.png" alt="MY ALT TEXT" width="100%" />
                <!-- <video poster="" id="video1" autoplay controls muted loop height="100%">
                <source src="assets/videos/stability_verification.mp4" type="video/mp4">
              </video> -->
                <figcaption class="video-caption">
                  Our goal is to identify <em><span style="font-weight: bold;">which</span></em> object has moved
                  between two consecutive images and <em><span style="font-weight: bold;">how</span></em> the geometric
                  relationship of objects has changed, while simultaneously inferring the semantic properties of each
                  object.
                  To this end, we present Visual Reasoning Descriptor (VRD) which translates input images into natural
                  language scene descriptions (referred to as visual residuals).
                  Visual residual \(V\) contains both the semantic properties of the objects and the difference in the
                  objects' configurations between consecutive image pairs and consists of three components:
                  $$\{l^{\text{semantic}}, l^{\text{geometric}}, l^{\text{description}}\}$$.
                </figcaption>
              </figure>
            </div>
          </div>

          <!-- Second Column -->
          <div class="column is-half">
            <h2 class="title is-3">Preference Reasoning Descriptor</h2>
            <div class="item item-video1">
              <figure class="video-container">
                <img src="assets/images/fig-prd.png" alt="MY ALT TEXT" width="100%" />
                <!-- <video poster="" id="video1" autoplay controls muted loop height="100%">
                <source src="assets/videos/receptacle_reasoning.mp4" type="video/mp4">
              </video> -->
                <figcaption class="video-caption">
                  To interpret the overall preference from the obtained sequence of visual residuals \(
                  \mathcal{V}=\{V_{1},\cdots,V_{n-1}\} \) between an image sequence \( \mathcal{I} \) of length \(n\),
                  we propose Preference Reasoning Descriptor (PRD) to interpret user preferences described in natural
                  language descriptions.
                  To this end, we propose Preference Reasoning Descriptor (PRD) to interpret user preferences described
                  in natural language descriptions.
                  The visual residual information (obtained from VRD) along with the original image sequence is fed into
                  PRD to reason about the underlying human preferences.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
  </section>
  <!-- End of module explanations -->


  <!-- Simulation Experimental Videos -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">User Interaction</h2>
      <div id="module">
        <div id="sim-results" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="assets/videos/sim-shelf-level1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="assets/videos/sim-shelf-level2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="assets/videos/sim-shelf-level3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End of simulation experimental videos -->

  <!-- User Interaction Videos -->
  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">User Interaction</h2>
        <div id="module">
          <div id="sim-results">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <source src="assets/videos/user_interaction.mp4" type="video/mp4">
            </video>
            <h2 class="content has-text-justified" style="text-align: left;">
              User interaction with the proposed system SPOTS. The user selects among the candidates, provided with a
              close consideration of stability and reasonableness, in an interactive viewer. SPOTS recommends the
              placement candidates based on the prompt of the task.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End of user interaction videos -->

  <!-- Environments -->
  <section class="section" style="margin-bottom: 120px;">

    <div class="container">
      <h2 class="title is-3">Environments</h2>
      <h2 class="content has-text-justified" style="text-align: left; position: absolute;">
        <p>
          To validate the effectiveness of VRD, we conducted a set of experiments focusing on visual reasoning
          performance.
          In addition, we designed further experiments to evaluate the ability to infer human preferences, which we
          divided into two categories: those based on semantic property and those based on spatial patterns.
          Specifically, the performance of our proposed method is evaluated across three different environments: Block
          Task, Polygon Task, and Household Task.
        </p>
      </h2>
    </div>
  </section>
  <!-- End environments -->

  <!-- Baselines & Metrics -->
  <section class="hero is-small" style="margin-bottom: 35px;">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Baselines & Metrics</h2>
        <div id="Baselines & Metrics">
          <p> We compare our method with other baselines, including large language models and a linear preference
            extractor. For fair comparisons on visual reasoning, we utilize the same visual reasoning module (i.e.,
            GPT-4V).</p>
          <ul>
            <li><strong>MLLM-Naive:</strong> An ablation of our approach that does not use the Visual Reasoning
              Descriptor and Preference Reasoning Descriptor. MLLM-Naive infers scene descriptions for consecutive image
              pairs in a similar way to our method but without using the VRD template. Then, this baseline interprets
              the preference directly, using only an entire image sequence in a single interaction.</li>
            <li><strong>MLLM-L2R:</strong> Inspired by Language-to-Reward (L2R), this baseline extracts normalized
              object 2D position (ranging from 0.0 to 1.0) information for feature computation. Subsequently, we
              integrate a code snippet generation module that produces a piece of code to compute preference weights
              using the obtained object positions.</li>
            <li><strong>Mutual-Distance-based Preference Extractor (MDPE):</strong> This baseline assumes that human
              preferences are deterministic, following a linear user model as discussed in prior works. Within the
              framework of linear models, MDPE computes the preference weights for each specific feature based on
              pre-defined functions using the mutual distances between objects and then derives the preference from
              these weights.</li>
          </ul>

          <p>Our evaluation metrics are the success rate of Visual Reasoning Descriptor (SR<sub>VRD</sub>) and the
            success rate of Preference Reasoning Descriptor (SR<sub>PRD</sub>) for the given image sequences. In
            particular, SR<sub>VRD</sub> is calculated based on the visual residual between the image sequences and is
            defined as follows:</p>
          <p>$$\text{SR}_\text{VRD} = \frac{1}{N-1} \sum_{k=1}^{N-1} \left( \frac{\sum_{l \in V_k} \mathbb{I}(l =
            \hat{l})}{|V_k|} \right)$$</p>
          <p>where \(| \cdot |\) means the number of elements in the set and \(\mathbb{I}\) represents an indicator
            function that checks whether each element \(l\) within the predicted response \(V_k\) matches its
            corresponding element \(\hat{l}\) in the ground truth visual residual \(\hat{V}_k\) for each consecutive
            image pair. The elements of \(V_k\) and \(\hat{V}_k\) include (\(l^{\text{semantic}}_k\),
            \(l^{\text{geometric}}_k\), \(l^{\text{description}}_k\)), and their respective ground truth counterparts
            (\(\hat{l}^{\text{semantic}}_k\), \(\hat{l}^{\text{geometric}}_k\), \(\hat{l}^{\text{description}}_k\)).</p>
          <p>On the other hand, SR<sub>PRD</sub> is measured according to the predicted preference that matches the
            ground truth. The preference criteria for each scene are manually designed. We formulate SR<sub>PRD</sub> as
            follows:</p>
          <p>$$\text{SR}_\text{PRD} = \frac{1}{M} \sum_{i=1}^{M} \mathbb{I}(l^{\text{preference}}_{i} =
            \hat{l}^{\text{preference}}_{i})$$ </p>
          <p>where the indicator function \(\mathbb{I}\) checks for a match between predicted description and ground
            truth preferences. SR<sub>PRD</sub> evaluates whether the predicted preferences
            \(l^{\text{preference}}_{i}\) are in alignment with the ground truth preferences
            \(\hat{l}^{\text{preference}}_{i}\) across a defined set of scenes.</p>
        </div>
      </div>
    </div>
  </section>
  <!-- End of Baselines & Metrics -->

  <!-- Results -->
  <section class="hero is-small" style="margin-bottom: 35px;">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Results</h2>
        <div id="Results">
          <div class="image-row">
            <img src="assets/images/fig-vpi-table1.png" alt="Image 1" class="image" data-href="#image1">
            <!-- <figcaption>Table 1</figcaption> -->
            <img src="assets/images/fig-vpi-table2.png" alt="Image 2" class="image" data-href="#image2">
            <!-- <figcaption>Table 2</figcaption> -->
            <img src="assets/images/fig-vpi-table3.png" alt="Image 3" class="image" data-href="#image3">
            <!-- <figcaption>Table 3</figcaption> -->
            <img src="assets/images/fig-vpi-table4.png" alt="Image 4" class="image" data-href="#image4">
            <!-- <figcaption>Table 4</figcaption> -->
          </div>
          
          <h1 style="font-size: 24px;">Block Task</h1>
          Firstly, Table <a href="#exp:visual-reasoning">1</a> compares the visual reasoning performance of our method against the ablation of our method.
          The results indicate that our method outperforms the naive approach in understanding both the semantic and
          geometric properties in between an image sequence.
          Especially the results of \(0.72 \pm 0.11\) demonstrate the superior visual reasoning ability of our method.
          In contrast, the MLLM-Naive model shows limited ability in extracting visual signals between images with
          \(\text{SR}_{\text{VRD}}\) of \(0.40 \pm 0.15\) for the same task.
          This result highlights the effectiveness of our VRD template-based approach in recognizing the visual
          residuals within image sequences.

          From the results in Table <a href="#exp:preference-reasoning">2</a>, we compare the spatial pattern preference
          reasoning performance of our method to three other baseline approaches.
          Our method shows outstanding reasoning preference performance in the Block Task.
          This highlights the benefits of our prompting method as compared to the other baselines.
          Although the MDPE approach was expected to get a perfect score, MDPE did not achieve a score of \(1.0\)
          (indicating that this model is always correct).
          This poor performance of MDPE is mainly due to the parameter sensitivity used in its function function.
          Such sensitivity often leads to the recognition of multiple preferences, resulting in erroneous preference
          inferences.
          MLLM-L2R also exhibits limited effectiveness primarily because relying solely on the responses of the MLLM for
          position information is impractical; they do not account for specific geometric locations.
          From the experiment results, we would like to emphasize that our method has a high potential for visual
          reasoning tasks, taking into account spatial pattern preferences.

          <br>
          <br>
          <h1 style="font-size: 24px;">Polygon Task</h1>
          On the Polygon task, as detailed in Table <a href="#exp:visual-reasoning">1</a>, our methodology significantly outperforms the baseline model, indicating superior visual reasoning accuracy.
          Specifically, our approach achieves a visual reasoning accuracy of \( 0.79 \pm 0.13 \) while the baseline records a lower accuracy of \(0.56 \pm 0.23\).
          This performance gap shows an enhanced ability of our method to accurately predict visual contexts within image sequences, especially in tasks involving complex geometric shapes such as polygons.
          
          The results in Table <a href="#exp:preference-reasoning">3</a> show that our method consistently outperforms other MLLM-based approaches for semantic preference reasoning in the Polygon task.
          In comparison, the MLLM-Naive method performs poorly with \(0.20\) for color and \(0.70\) for shape, indicating the limitations of naive models in capturing semantic properties.
          On the other hand, the MLLM-L2R model shows slight improvements, achieving a score of \(0.30\) for color and \(0.80\) for shape.
          However, these results still do not reach our reasoning preference performance.
          Notably, MDPE achieves the perfect scores (=\(1.0\)) both on color and shape criteria. 
          However, it is important to know that the performance of MDPE depends on the manual tuning of the preference feature functions.
          These results imply that our method can successfully capture semantic preferences without any manual feature engineering, such as predefining a list of object attributes.

          <br>
          <br>
          <h1 style="font-size: 24px;">Household Task</h1>
          The metric of \(\text{SR}_{\text{VRD}}\), presented in Table <a href="#exp:visual-reasoning">4</a> compared the visual reasoning performance of our approach against the ablation of our method, which was evaluated six times respectively.
          In particular, the results of \(0.63 \pm 0.08\) demonstrated the higher visual reasoning performance of our method.
          In contrast, the MLLM-Naive model showed a limited ability to extract visual signals between images with \(\text{SR}_{\text{VRD}}\) of \(0.28 \pm 0.19\) for the same task. 
          These results support the effectiveness of our VRD template-based approach in recognition of visual residuals within image sequences.
          
          Each type of preference was evaluated six times, and performance was measured in terms of \(\text{SR}_{\text{PRD}}\).
          As illustrated in Fig. <a href="#framework">1</a> in each step, the robot performs to move objects and captures images.
          The results of our method in Table <a href="#exp:preference-reasoning">4</a> are consistent with our simulation results, indicating the balanced performance of our method in spatial pattern and semantic preference reasoning. 
          Compared to other MLLM-based approaches, they showed subpar performance in recognizing spatial patterns and semantic properties. 
          We can notice that MLLM tends to misunderstand the spatial arrangements or semantic properties of objects without explicit annotation by VRD.
          While MDPE performs as effectively as our approach for both types of preference, it remains highly dependent on the need for handcrafted features.
          These results support the practical effectiveness of our method and support its successful application in real-world scenarios.          
        </div>
      </div>
    </div>
  </section>
  <!-- End of Results -->


  <!-- Real world Experimental Videos -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Real World Demonstration</h2>
        <div id="module">
          <div id="realworld">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="assets/videos/real_world.mp4" type="video/mp4">
            </video>
            <h2 class="content has-text-justified" style="text-align: left;">
              In this experiment, we consider a scene with different objects placed on a desk.
              We designed a task that categorizes objects based on similarity. The reasoning criteria, termed
              <strong>similarity</strong>, varies for each experiment and serves as the ground truth for evaluating
              reasoning abilities.
              Each type of <strong>similarity</strong> was evaluated five times, and performance was measured using the
              overall success rate (i.e., both Sta. S/R and Rea. S/R).
              From this experiment, we insist that the reasonable place varies depending on the task description given
              as input. Furthermore, we are able to accurately determine the stable positions to place the objects by
              reconstructing the robot's ego-centric view with the real-to-sim method.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End of Real world experimental videos -->


  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
  <!-- Paper video. -->
  <!-- <h2 class="title is-3">Video</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
  <!-- Youtube embed code here -->
  <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End youtube video -->

  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="assets/pdfs/2024_IROS_VPI_v04.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{lee2024visual,
          title={Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation}, 
          author={Joonhyung Lee and Sangbeom Park and Yongin Kwon and Jemin Lee and Minwook Ahn and Sungjoon Choi},
          year={2024},
          eprint={2403.11513},
          archivePrefix={arXiv},
          primaryClass={cs.RO}
        }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

<script>

  timeoutIds = [];

  function populateDemo(imgs, num) {
    // Get the expanded image
    var expandImg = document.getElementById("expandedImg-" + num);
    // Get the image text
    var imgText = document.getElementById("imgtext-" + num);
    var answer = document.getElementById("answer-" + num);

    // Use the same src in the expanded image as the image being clicked on from the grid
    expandImg.src = imgs.src.replace(".png", ".mp4");
    var video = document.getElementById('demo-video-' + num);
    // or video = $('.video-selector')[0];
    video.pause()
    video.load();
    video.play();
    video.removeAttribute('controls');

    console.log(expandImg.src);
    // Use the value of the alt attribute of the clickable image as text inside the expanded image
    var qa = imgs.alt.split("[sep]");
    imgText.innerHTML = qa[0];
    answer.innerHTML = "";
    // Show the container element (hidden with CSS)
    expandImg.parentElement.style.display = "block";
    for (timeoutId of timeoutIds) {
      clearTimeout(timeoutId);
    }

    // NOTE (wliang): Modified from original to read from file instead
    fetch(qa[1])
      .then(response => response.text())
      .then(contents => {
        // Call the processData function and pass the contents as an argument
        typeWriter(contents, 0, qa[0], num);
      })
      .catch(error => console.error('Error reading file:', error));
  }

  function typeWriter(txt, i, q, num) {
    var imgText = document.getElementById("imgtext-" + num);
    var answer = document.getElementById("answer-" + num);
    if (imgText.innerHTML == q) {
      for (let k = 0; k < 5; k++) {
        if (i < txt.length) {
          if (txt.charAt(i) == "\\") {
            answer.innerHTML += "\n";
            i += 1;
          } else {
            answer.innerHTML += txt.charAt(i);
          }
          i++;
        }
      }
      hljs.highlightAll();
      timeoutIds.push(setTimeout(typeWriter, 1, txt, i, q, num));
    }
  }

  document.addEventListener('DOMContentLoaded', function() {
    var images = document.querySelectorAll('.image');
    images.forEach(function(image) {
      image.addEventListener('click', function() {
        var href = this.getAttribute('data-href');
        if (href) {
          window.location.href = href;
        }
      });
    });
  });
</script>

</html>